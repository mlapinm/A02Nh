### Reinforcement Learning With Q-Learning: Example

So now that we kind of understand that I want to move on to a q-learning example. And what we're gonna do for this example is actually use something called the open AI gym. Now I just need to throw my drawing tab of the way right there so that we can get started. But open AI gym is actually a really interesting kind of module I don't even actually I don't even really know the way to describe it. Almost tool there's actually developed by open AI I'm you know coincidentally by the name which is founded by Elon Musk and someone else so. He's actually you know made this kind of. 

I really don't know the word to describe it. I almost want to say tool that allows programmers to work with these really cool GM environments and train reinforcement learning models. So you'll see how this works in a second. But essentially there's a ton of graphical environments that have very easy interfaces to use so like moving characters around them that you're allowed to experiment with completely for free as a programmer to try to you know make some cool reinforcement learning models. That's what opening a gym is and you can look at it. I mean we'll click on it here actually to see what it is you can see. Jim there's all these different Atari environments and just a way to kind of train reinforcement learning modes all right so now we're gonna start by just importing Jim. 

If you're in collaboratory there's nothing you need to do here if you're in your own thing you're gonna have to pip install Jim. And then what we're gonna do is make this frozen like v-0 Jim so essentially what this does is just set up the environment that we're gonna use. Now I'll talk more about what this environment is later but I want to talk about how Jim works because we are gonna be using this throughout. The thing so the open AI Jim is meant for reinforcement learning and essentially what it has is an observation space and an action space for every environment. 

Now the observation space is what we call our environment right and that will tell us the amount of states that exist in this environment now in our case we're gonna be using kind of like a maze-like thing which I'll show you in a second. 

So you understand why we get the values we do. Action space tells us how many actions we can take when we do the dot n at any given state so if we print this out we get 16 and for representing that the observation space. In other words is the number of states is 16 and the amount of actions we can take and every single state is 4 now in this case these actions gonna be left down up and right but yes now Ian V dot reset so essentially we have some commands that allow us to move around the environment which are actually down here. If we want to reset the environment and start back in the beginning state then we do Ian's V dot reset. You can see this actually returns to us the starting state which obviously is going to be zero. Now we also have the ability to take a random action or select a random action from the action space. 

So what this line does right here is say of the action space so of all the commands that are there or all the actions we could take pick a random one and return that so if you do that actually let's just print action and see what this is. You see we get zero two right it just gives us a random action that is valid from the action space. Alright next what we have is this env dot step in action now what this does is take whatever action we have which in this case is three and perform that in the environment so tell our agent to take this action in the environment and return to us. 

A bunch of information so the first thing is the observation which essentially means what state do we move into next. So I could call this new under source state reward is what reward did we receive by taking that action so this will be some value right and our in this case the reward is either 1 or 0 but that's not that important to understand. And then we have a pool of done which tells us did we lose the game or did we win the game yes or no. So true so if this is true what this means is we need to reset the environment because our agent either lost or won and is no longer in a valid state in the environment info gives us a little bit of information. 

It's not showing me anything here we're not gonna use info throughout this but figured I'd let you know that now Ian V don't render. I'll actually render this for you and show you renders a graphical user interface that shows you the environment now. 

If you use this while you're training so you actually watch the age and do the training which is what you can do with this it slows it down drastically like probably by you know 10 or 20 times because it actually needs to draw the stuff on the screen. But you know you can use it if you want so this is what our frozen like example looks like. 

You can see that the highlighted square where our agent is and in this case we have four different blocks. We have SF hng so S stands for start F stands for frozen because this is a frozen lake and the goal is to navigate to the goal without falling in one of the holes which is represented by H. And this here tells us the action that we just took now. I guess the starting action is up because that's zero I believe but yes so if we run this a bunch of times. We'll see this updating unfortunately this doesn't work very well in Google collaboratory the the gooeys. But if you did this in your own command line and you like did some different steps and rounded it all out you would see this working properly okay. So now we're on to talking about the frozen lake environment which is kind of what I just did so now we're just gonna move to the example where we actually implement cue learning to essentially solve the problem. How can we train the AI to navigate this environment and get to the start to the goal. How can we do that what we're gonna use cue learning. So let's start so the first thing we need to do is import Jim import numpy and then create some constants here. So we'll do that we're gonna say the amount of states is equal to the line I showed you before so en veto observation space dot and actions is equal to e NV dot action space N. And we're gonna say Q is equal to NP zeroes States and actions so something. 

I guess I forgot to mention is when we initialize the cue table we just initialize all blank values or 0 values. Because obviously at the beginning of our learning our model or our agent doesn't know anything about the environment yet. So we just leave those all blank which means we're gonna more likely be taking random actions at the beginning of our training trying to explore the environment space more and then as we get further on and learn more about the environment. Those actions will likely be more calculated based on the cue table values so we print this out we can see this is the array that we get we've had to be build a 16 by 4. I guess not array why is this technically isn't. A cute line we'll called matrix 16 by 4 so every single row represents a state and every single column represents an action that could be taken in that state. 

All right so we're going to find some constants here which we talked about before. So we have the gamma the learning rate the max amount of steps in the number of episodes. So the number of episodes is actually how many episodes you want to train your agent on so how many times you want it to run around and explore the environment that's what episode stands for max steps essentially says okay. So if we're in the environment and we're kind of navigating and moving around we haven't died yet how many steps are we gonna let the agent take before we cut it off because what could happen is we could just bounce in between two different states indefinitely. So we need to make sure we have a max steps so that at some point if the agent is just doing the same thing we can you know end that. 

Or if it's like going in circles we can end that and and start again with different you know Q values. All right so episodes yeah. We already talked about that learning rate we know what that is gamma we know what that is mess with these values as we go through and you'll see the difference it makes in our training. I've actually include a graph down below so we'll talk about that you kind of show us the outcome of our training but learning rate the higher this is the faster I believe that it learns. Yes so a high learning rate means that each update will introduce larger change to the current state. So yeah so that makes sense based on the equation as well just want to make sure that I wasn't going crazy there so let's run this constant block to make sure. 

And now we're gonna talk about picking an action so remember how I said and I actually wrote them down here there's essentially two things we can do at every what we call it step right we can randomly pick a valid action where we can use the current Q table to find the best action. So how do we actually implement that into our open a I'd Jim well I just want it to write a little code block here to show you the exact code that will do this for us. So we're gonna introduce this new concept or this new you can almost call it constant called Epsilon. And I think Epsilon I think I spelled this wrong Epsilon yeah that should be how you spell it. 

So we're gonna start the epsilon value essentially tells us the percentage chance that we're gonna pick a random action so here we're gonna use a 90% Epsilon which essentially means that every time we take an action there's gonna be a 90% chance that's random and 10% chance that we look at the cute able to make that action. 

Now we'll reduce this epsilon value as we trained so that our model will start being able to explore you know as much as it possibly can in the environment by just taking random actions and then after we have enough observations we've explored the environment. Enough we'll start to slowly decrease the epsilon so that it hopefully finds a more optimal route for things to do now the way we do this is we save NP random uniform 0 1 which essentially means pick a random value between 0 & 1 is less than epsilon and epsilon. Like that's I think I'm gonna have to change some other stuff but we'll see then action equals NV dot action spaced out samples so take a random action. That's what this means store what that action is in here. 

Otherwise we're gonna take the argument max of the state row in the Q table so what this means is find the maximum value in the Q table and tell us what row it's in so that way we know what action to take. So if we're in row I guess that's all right not row column 4 in column 1 you know that's the maximum value take action 1 that's what this is saying so using a cute table to pick the best action. Alright so we don't need to run this because this is just gonna be which I just wrote that to show you now how did we update the Q values. Well this is just following the equation that I showed above so this is the line of code that does this I just want to write it out so you guys could see exactly what each line is doing and kind of explore it for yourself. 

But essentially you get the point you know you have your learning rate reward gamma take the max so NP dot max does the same thing as a max function in Python. This is gonna take the max value not the argument max from the next state writes the new state that we moved into and then subtracting obviously the Q state action. All right so putting it all together so now we're actually going to show how we can train and create this Q table and then use that Q table. So this is the pretty much all this code that I have we've already actually written. At least this block here that's why I put it in its own block. So just all the constants I've included this render constant to tell us whether we want to draw the environment or not. In this case I'm gonna leave it false but you can make it true if you want episodes. I've left out 1500 for this if you want to make your model better typically you train it on more episodes but that's up to you. 

And now we're gonna get into the big chunk of code which I'm going to talk about so what this is gonna do I'm gonna have a rewards list which is actually just gonna store all of the rewards we see just. So I can graph that later for you guys then we're gonna say for episode in range episodes so just telling us you know for every episode let's do the steps. I'm about to do so maximum amount episodes which is our training length. Essentially we're gonna reset the state obviously which makes sense so state equals e'en betta reset which will give us the starting state we're gonna say four underscore in range max steps which means okay. We're gonna do you know we're gonna explore the environment up to maximum steps we do have a done here which will actually break the loop if we breach the goal which we'll talk about further. 

So the first thing we're gonna do is say if render you know render the environment that's pretty straightforward otherwise let's take an action so for each time step. We need to take an action so epsilon. I think is spelled correctly here ya believe that's right. I'm gonna say action equals e MV dot action space this is already the code we've looked at. And then we're gonna say is next state reward done underscore equals e MV dot step action we've put an underscore here. Because we don't really care about this info value so I'm not going to store it but we do care about what the next state will be the reward from that action and if we were done or not so we take that action. 

That's what does this MV step. And then what we do is say Q state action and we just update the Q value using the formula that we've talked about. So this is the formula you can look at it more in depth if you want but based on whatever the reward is you know that's how we're going to update. Those Q values and after a lot of training we should have some decent Q values in there. All right so then we set the current state to be the next state. So that when we run this time step again now our agent is in the next state and can start exploring the environment again in this current you know iteration almost if that makes sense. So then we say if done so essentially if the agent died or if they lost or with whatever it was. We're going to append whatever reward they got from their last step into the rewards up here. 

And it's worthy of noting that the way the rewards work here is you get one reward if you move to a valid block and you get the zero award if you die. So every time we move to a valid spot we get one otherwise we get zero. I'm pretty sure that's the way it works at least but that's something that's important to note so then what we're gonna do is reduce the Epsilon if we died by just a fraction of an amount you know 0.001. Just so we slow we start decreasing the epsilon moving in the correct direction and then we're gonna break because we've reached the goals print the cue table and then print the average reward now this takes a second to Train like you know a few seconds. Really that one is pretty fast because I've set this at was it fifteen hundred but if you want you can set this out say ten thousand. 

Wait another you know a few minutes or whatever and then see how much better you can do so we can see that after that I received an average reward of zero point two eight eight eight six six six six seven. This is actually what the cue table values look like so all these decimal values after all these updates I just decided to print them out and I just want to show you the average reward so that we can compare that to what we can get from testing or this graph. So now I'm just gonna graph this and we're gonna see this is what the graph so you don't have to really understand this code if you don't want to but this is just graphing the average reward over a hundred steps from the beginning to the end. 





So essentially I've been I've calculated the average of every a hundred episodes and then just graph this on here we can see that we start off very poorly in terms of reward because the epsilon value is quite high. Which means that we're taking you know random actions pretty much all the time so if we're taking a bunch of random actions obviously chances are we're probably gonna die a lot. We're probably gonna get rewards of zeroes quite frequently and then after we get to about 600 episodes you can see that six actually represents 600. Because this isn't hundreds we start to slowly increase and then actually we go on a crazy increase here when we start to take values more frequently so the epsilon is increasing right. And then after we get here we kind of level off and I this does show a slight decline. 

But I guarantee you if we ran this for you know like fifteen thousand it would just go up and down and bob up and down and not just because even though we have increased the epsilon there is still a chance that we take a random action. And you know gets your reward so that is pretty much it for this Q learning example and I mean that's pretty straightforward to use the Q table if you actually want it to say you know watch the agent move around the thing. I'm gonna leave that to you guys because if you can follow what I've just done in here and understand this it's actually quite easy to use the Q table. And I think as like a final almost like you know trust in you guys you can figure out how to do that the hint is essentially do exactly what I've done in here.

Don't update the cue table values just use the cue table values already and that's you know pretty much all there is to cue learning so this has been the reinforcement learning module for this tensorflow course which actually is the last module in this series. Now I hope you guys have enjoyed up until this point just an emphasis again this was really just an introduction to reinforcement learning this technique in this problem itself is not very interesting and not you know the best way to do things it's not the most powerful. It's just to get you thinking about how reinforcement learning works and potentially if you'd like to look into that more there's a ton of different resources and you know things you can look at in terms of reinforcement learning so that being said that has been this module. And now we're gonna move into the conclusion we'll talk about some next steps and some more things that you guys can look at to improve your machine learning skills. 







