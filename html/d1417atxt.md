Essentially we can see here is that if we're saying X is our input and H is our output X T is going to be our input at time T whereas HT is going to be our output at time T if we had a text of say length four so for words like we've encoded them into integers now at this point the first input at time zero will be the first word into our network right or the first word that this layer is going to see and then the output at that time is going to be our current understanding of the entire text after looking at just that one word next what we're gonna do is process input one which will be the next word in the sentence but we're gonna use the output from the 

previous kind of computation or the previous iteration to do this so we're gonna process this word in combination with what we've already seen and then have a new output which hopefully should now give us an understanding of what those two words mean next we'll go to the third word and so forth and slowly start building our understanding of what the entire textual data means by building it up one by one the reason we don't pass the entire sequence at once is because it's very difficult to just kind of look at this huge blob of integers and figure out what the entire thing means if we can do it one by one and understand the meaning of specific words based on the words that have came before it and start learning those patterns that's gonna be a lot easier for our neural network to deal with than just passing it all at once looking at it and trying to get some output and that's why we have these recurrence layers there's a few different types of them and I'm gonna go through them and then we'll talk a little bit more in depth of how they work so the first one is called long short term memory and actually in fact before we get into this let's let's talk about just a firt like a simple layer so that we kind of have a reference point before going here okay so this is kind of the example i want to use here to illustrate however current neural network works and a more I don't teaching style rather than what I was doing before so essentially the way that this works is that this whole thing that I'm drawing here right all of this circle stuff is really one layer and what I'm doing right now is breaking this layer apart and showing you kind of how this works in a series of steps so rather than passing all the information at once we're gonna pass it as a sequence which means that we're gonna have all these different words and we're gonna pass them one at a time to the kind of to the layer right to this recurrent layer so we're gonna start from this left side over here so let's write you know start over here at time step zero that's what zero means so time step is just you know the order in this case this is the first word so let's say we have the sentence hi I am Tim right we've broken these down into vectors they've been turned into their numbers I'm just writing them here so we can kind of see what I mean in like a natural language and they are the input to this recurrent layer so all of our different words right that's how many kind of little cells we're gonna draw here is how many words we have in this sequence that we're talking about so in this case we have 4 right 4 words so that's why I've drawn 4 cells to illustrate that now what we do is that time step 0 the internal state of this layer is nothing there's no previous output we haven't seen anything yet which means that this first kind of cell which is what I'm looking at right here what I'm drawing in this first cell is only going to look and consider this first word and kind of make some prediction about it and do something with it we're gonna pass hi to this sell some maths gonna go on in here and then what it's gonna do is it's gonna output some value which you know tells us something about the word hi write some numeric value we're not gonna talk about what that is but it's gonna be there's gonna be some output now what happens is after this cell has finished processing this survey so this one's done this is completed h0 the outputs there we'll do a check mark to say that that's done it's finished processing this output gets fed into actually the same thing again we're kind of just keeping track of it and now what we do is we process the next input which is I and we use the output from the previous cell to process this and understand what it means so now technically we should have some output from the previous cell so from whatever high was right we do some analysis on the word I we kind of combine these things together and that's the output of this cell is our understanding of not only the current input but the previous input with the current input so we're slowly kind of building up our understanding of what this word I means based on the words we saw before and that's the point I'm trying to get at is that this network uses what it's seen previously to understand the next thing that it sees it's building a context it's trying to understand not only the word but what the word means you know in relation to what's come before it so that's what's happening here so then this output here right we we get some output we finish this we get some output h1 h1 is passed into here and now we have the understanding of what hi and I means and we add am like that we do some kind of computations we build an understanding what this sentence is and then we get the output h2 that passes to h3 and now finally we have this final output h3 which is gonna understand hopefully what this entire thing means now this is good this works fairly well and this is called a simple RNN layer which means that all we do is we take the output from the previous cell of the pre iteration because really all of these cells is just an iteration almost in a four loop right based on all the different words in our sequence and we slowly start building to that understanding as we go through the entire sequence now the only issue with this is that as we have a very long sequence so sequences of length say 100 or 150 the beginning of those sequences starts to kind of get lost as we go through this because remember all we're doing right is the output from each two is really a combination of the output from H 0 and H 1 and then there's a new word that we've looked at and H 3 is now a combination of everything before it and this new word so it becomes increasingly difficult for our model to actually build a really good understanding of the text in general when the sequence gets long because it's hard for it to remember what it's seen at the very beginning because that is now so insignificant there's been so many outputs tacked on to that that it's hard for it to go back and and that if that makes any sense ok so what I'm gonna do now is try to explain the next layer we're gonna look at which is called LS TM so the previous layer we just looked at the recurrent layer was called a simple r n n layer so simple recurrent neural network layer whatever you want to call it right simple recurrent layer now we're gonna talk about the layer which is LS TM which stands for long short term memory now long and short are hyphenated together but essentially what we're doing and it just gets a little bit more complex but I won't go into the math is we add another component that keeps track of the internal state so right now the only thing that we were tracking as kind of our internal state as the memory for this model was the previous output so whatever the previous output was so for example at time 0 here there was no previous output so there was nothing being kept in this model but at time 1 the output from this cell right here was what we were storing and then at cell 2 the only thing we were storing was the output at time 1 right and we've lost now the output from time 0 what we're adding in long short-term memory is an ability to access the out from any previous state at any point in the future when we want it now what this means is that rather than just keeping track of the previous output we'll add all of the outputs that we've seen so far into what I'm gonna call my little kind of conveyor belt it's gonna run at the top up here I know it's kind of hard to see but it's just what I'm highlighting it's almost just like a lookup table that can tell us the output at any previous cell that we want so we can kind of add things to this conveyor belt we can pull things off we can look at them and this just adds a little bit of complexity to the model it allows us to not just remember the last state but look anywhere at any point in time which can be useful now I don't want to go into much more depth about exactly how this works but essentially you know just think about the idea that as the sequence gets very long it's pretty easy to forget the things we saw at the beginning so if we can keep track of some of the things we've seen at the beginning and some of the things in between on this little conveyor belt and we can access them whenever we want then that's gonna make this probably a much more useful layer right we can look at the first sentence and the last sentence of a big piece of text at any point that we want and say okay you know this tells us X about the meaning of this text right so that's what this LST M does again I don't want to go too far we've already spent a lot of time kind of covering you know recurrent layers and how all this works anyways you do want to look it up some great mathematical definitions again I will source everything at the bottom of this document so you can go there but again that's lsdm long short-term memory that's what we're gonna use for some of our examples although simple RNN does work fairly well for shorter length sequences and again remember we're treating our text as a sequence now where we're gonna feed each word into the recurrent layer and it's gonna slowly start to develop an understanding as it reads through each word random processes that
