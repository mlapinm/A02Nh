### TensorFlow - Natural Language Processing With RNNs: Recurring Neural Networks
### Natural Language Processing With RNNs C - TensorFlow 2.0 Course

Okay so now that we've talked about kind of the form that we need to get our data in before we can pass it further in the neural network right. Before it can get past that embedding layer before it can get put in put into any dense neurons before we can even really do any math with it we need to turn it into numbers right our textual data. So now that we know that it's time to talk about recurrent neural networks. Now recurrent neural networks are the type of networks we use when we process textual data. 


Typically you don't always have to use these. But they are just the best for natural language processing. And that's why they're kind of their own class right now the fundamental difference between a recurrent neural network and something like a dense neural network or a convolutional neural network is the fact that it contains an internal loop. Now what this really means is that the recurrent neural network does not process our entire data at once. So it doesn't process the entire training example or the entire input to the model at once. What it does is processes it at different time steps. 

And maintains what we call an internal memory and kind of an internal state. So that when it looks at a new input it will remember what it's seen previously and treat that input based on kind of the context of the understanding it's already developed. Now I understand that this doesn't make any sense right now. But with a dense neural network or the neural networks we looked at so far we call those something called called feed-forward neural networks. What that means is we give all of our data to it at once and we pass that data from left to right. Your I guess for you guys from left to right so we give all of the information.

You know we would pass those through the convolutional layer to start maybe we have passed them through dense neurons but they get given all of the info and then that information gets translated through the network to the very end again from left to right. Whereas here with recurrent neural networks we actually have a loop which means that we don't feed the entire textual data at once. We actually feed one word at a time it processes that word generate some output based on that word and uses the internal memory state that is keeping track of to do that as part of the calculation. 

So essentially the reason we do this is because just like humans when we you know look at text we don't just take a photo of this text and process it all at once we read it left to right word to word. And based on the words that we've already read we start to slowly develop an understanding of what we're reading right. If I just read the word now that doesn't mean much to me. If I just read the word in code that doesn't mean much whereas if I read the entire sentence. Now that we've learned a little bit about how we can encode text I start to develop an understanding about what this next word means based on the previous words before it right. 

And that's kind of the point here is that this is what a recurrent neural networks gonna do for us it's going to read one word at a time and slowly start building up its understanding of what the entire textual data means. And this works in kind of a more complicated sense than that we'll draw it out a little bit. But this is kind of what would happen if we unraveled a recurrent layer. Because recurrent neural network yes it has a loop in it. But really the recurrent aspect of a neural network is the layer that implements this recurrent functionality with a loop. 

Essentially we can see here is that if we're saying X is our input and H is our output. X T is going to be our input at time T whereas HT is going to be our output at time. T if we had a text of say length four so for words like we've encoded them into integers. Now at this point the first input at time zero will be the first word into our network right or the first word that this layer is going to see and then the output at that time is going to be our current understanding of the entire text. 

After looking at just that one word next what we're gonna do is process input one which will be the next word in the sentence. But we're gonna use the output from the previous kind of computation or the previous iteration. To do this so we're gonna process this word in combination with what we've already seen and then have a new output which hopefully should now give us an understanding of what those two words mean. Next we'll go to the third word and so forth and slowly start building our understanding of what the entire textual data means by building it up one by one. 

The reason we don't pass the entire sequence at once is because it's very difficult to just kind of look at this huge blob of integers and figure out what the entire thing means. If we can do it one by one and understand the meaning of specific words based on the words that have came before it and start learning those patterns. That's gonna be a lot easier for our neural network to deal with than just passing it all at once looking at it and trying to get some output. And that's why we have these recurrence layers there's a few different types of them. And I'm gonna go through them. And then we'll talk a little bit more in depth of how they work. So the first one is called long short term memory. 

And actually in fact before we get into this let's let's talk about just a firt like a simple layer. So that we kind of have a reference point before going here. Okay so this is kind of the example i want to use here to illustrate however current neural network works. And a more I don't teaching style rather than. What I was doing before so essentially the way that this works is that. This whole thing that I'm drawing here right all of this circle stuff is really one layer. And what I'm doing right now is breaking this layer apart and showing you kind of how this works in a series of steps. 

So rather than passing all the information at once we're gonna pass it as a sequence. Which means that we're gonna have all these different words and we're gonna pass them one at a time to the kind of to the layer right to this recurrent layer. So we're gonna start from this left side over here. So let's write you know start over here at time step zero that's what zero means so time step is just you know the order in this case. This is the first word. So let's say we have the sentence hi I am Tim right. We've broken these down into vectors they've been turned into their numbers. I'm just writing them here so we can kind of see what I mean in like a natural language. 

And they are the input to this recurrent layer. So all of our different words right that's how many kind of little cells we're gonna draw here is how many words we have in this sequence. That we're talking about so in this case we have 4 right 4 words so that's why I've drawn 4 cells to illustrate that now what we do is that time step 0 the internal state of this layer is nothing there's no previous output. We haven't seen anything yet which means that this first kind of cell which is what I'm looking at right here. What I'm drawing in this first cell is only going to look and consider this first word. 

And kind of make some prediction about it and do something with it. We're gonna pass hi to this sell some maths gonna go on in here. And then what it's gonna do is it's gonna output some value which you know tells us something about the word hi write some numeric value. We're not gonna talk about what that is. But it's gonna be there's gonna be some output. Now what happens is after this cell has finished processing. This survey so this one's done this is completed h0 the outputs there we'll do a check mark to say. That that's done. It's finished processing. This output gets fed into actually the same thing again. 

We're kind of just keeping track of it and now what we do is we process the next input which is I. And we use the output from the previous cell to process this and understand what it means. So now technically we should have some output from the previous cell. So from whatever high was right we do some analysis on the word. I we kind of combine these things together. And that's the output of this cell is our understanding of not only the current input but the previous input with the current input. So we're slowly kind of building up our understanding of what this word I means based on the words we saw before. 

And that's the point I'm trying to get at is that this network uses what it's seen previously to understand the next thing that it sees. It's building a context it's trying to understand not only the word but what the word means you know in relation to what's come before it. So that's what's happening here so then this output here. Right we we get some output we finish this we get some output h1 h1 is passed into here. And now we have the understanding of what hi. And I means and we add am like that we do some kind of computations we build an understanding what this sentence is. And then we get the output h2 that passes to h3 and now finally we have this final output h3. Which is gonna understand hopefully what this entire thing means now. This is good this works fairly well and this is called a simple RNN layer. 

Which means that all we do is we take the output from the previous cell of the pre iteration. Because really all of these cells is just an iteration almost in a four loop right based on all the different words in our sequence. And we slowly start building to that understanding as we go through the entire sequence. 

Now the only issue with this is that as we have a very long sequence. So sequences of length say 100 or 150 the beginning of those sequences starts to kind of get lost. As we go through this because remember all we're doing right is the output from each two is really a combination of the output from H 0 and H 1. 

And then there's a new word that we've looked at and H 3 is now a combination of everything before it. And this new word so it becomes increasingly difficult for our model to actually build a really good understanding of the text in general when the sequence gets long because it's hard for it to remember what it's seen at the very beginning. Because that is now so insignificant there's been so many outputs tacked on to that that it's hard for it to go back. 

And and that if that makes any sense ok so what I'm gonna do now is try to explain the next layer we're gonna look at which is called LS TM. So the previous layer we just looked at the recurrent layer was called a simple r n n layer so simple recurrent neural network layer whatever you want to call it right simple recurrent layer. Now we're gonna talk about the layer which is LS TM which stands for long short term memory. Now long and short are hyphenated together but essentially what we're doing and it just gets a little bit more complex. 

But I won't go into the math is we add another component that keeps track of the internal state. So right now the only thing that we were tracking as kind of our internal state as the memory for this model was the previous output. So whatever the previous output was so for example at time 0 here there was no previous output so there was nothing being kept in this model. 

But at time 1 the output from this cell right here was what we were storing. And then at cell 2 the only thing we were storing was the output at time 1. Right and we've lost now the output from time 0 what we're adding in long short-term memory is an ability to access the out from any previous state at any point in the future when we want. 

It now what this means is that rather than just keeping track of the previous output we'll add all of the outputs that we've seen so far into what I'm gonna call my little kind of conveyor belt.

It's gonna run at the top up here I know it's kind of hard to see but it's just what I'm highlighting it's almost just like a lookup table that can tell us the output at any previous cell that we want so we can kind of add things to this conveyor belt. We can pull things off we can look at them and this just adds a little bit of complexity to the model it allows us to not just remember the last state but look anywhere at any point in time which can be useful. 

Now I don't want to go into much more depth about exactly how this works but essentially you know just think about the idea that as the sequence gets very long it's pretty easy to forget the things we saw at the beginning. So if we can keep track of some of the things we've seen at the beginning and some of the things in between on this little conveyor belt and we can access them whenever we want then that's gonna make this probably a much more useful layer right. 

We can look at the first sentence and the last sentence of a big piece of text at any point that we want and say okay you know this tells us X about the meaning of this text right so that's what this LST M does again. I don't want to go too far we've already spent a lot of time kind of covering you know recurrent layers and how all this works anyways you do want to look it up some great mathematical definitions again. I will source everything at the bottom of this document so you can go there but again that's lsdm long short-term memory that's what we're gonna use for some of our examples. 

Although simple RNN does work fairly well for shorter length sequences. And again remember we're treating our text as a sequence now where we're gonna feed each word into the recurrent layer and it's gonna slowly start to develop an understanding. As it reads through each word random processes that.


