### Convolutional Neural Networks: Picking a Pretrained Model

All right so picking a pre-trained model. So this is the next step this is probably one of the harder steps is picking a model that you would actually like to use the base up. Now we're gonna use one called mobile net b2 which is actually from Google. It's built into tensorflow itself. That's why I've picked it. And all we're gonna do is set this. 


So essentially we're gonna say the base model in our code is equal to TF duck Kara's applications mobile on that v2. Which is just telling us the architecture of the model that we want. 

And we'll have a look at it down below here in just a second. We'll define the input shape which is important because this can take any input shape that we want. 

So we'll change it to 160 160 by 3 which we've defined up here include top very important means do we include the classifier that comes with this network already or not now in our case. We're gonna be retraining parts of this network so that it works specifically for dogs and cats. And not for a thousand different classes which is what this model was actually aimed to do is train a 1.4 million images for a thousand different classes of everyday objects. So we're gonna eight not include the top which means that don't include the classifier for these a thousand classes. And we're gonna load the weights from what's called image net which is just a specific save of the weights. So this is the architecture and this is kind of the data that we're filling in for that architecture so the weights and we'll load that in which we have here so base model. Now let's look at it. 

So let's have a summary. You can see this is a pretty crazy model. I mean we would never be expected to create something like this by ourself. This is you know teams of data scientists PhD students engineers what I write the experts in the field that have created a network like this. So that's why we're gonna use it. Because it works so effectively for the generalization at the beginning. Which is what we want. And then we can take those features that this takes out so in five by five by 1280. Which is what I want us to focus on the output of this actual network here. So really you can see this last layer. We're gonna take this and using this information pass that to some more convolutional layers and actually our own classifier I believe. And use that to predict versus dogs furs cats. So at this point the base model will simply open a shape thirty-two by five by five by twelve eighty that's the tensor that we're gonna get out of this. 

That's the shape you can watch how this kind of works as you go through it. Yes all right so we can just have a look at this year. This what I wanted to do essentially was just look at what the actual shape was gonna be so 32 5x5 by 1280 just because this gives us none until it knows what the input is. And now it's time to talk about freezing the base. So essentially the point is we want to use this as the base of our network which means we don't want to change. It if we just put this network in right now as the base to our neural network. 


Well what's gonna happen is it's gonna start retraining all these weights and biases. And in fact it's gonna Train two point two five seven million more weights and biases. When in fact we don't want to change these because these have already been defined. They've been set and we know that they work well for the problem already right. They works well for classifying a thousand classes. Why are we gonna touch this now. And if we were gonna touch this what's the point of even using this base. Right we don't want to train this. We want to leave it the same. So to do that we're just gonna freeze it now. 

Freezing is a pretty. I mean it just essentially means turning the trainable attribute of a layer off or of the model off. So what we do is you just say base model trainable equals false. Which essentially means that we are no longer gonna be training any aspect of that I want to say model. Although we'll just call it the base layer for now at the base model. So now if we look at the summary. We can see when we scroll down to the bottom. If we get there any day soon. That now the trainable parameters is zero instead of two point two five seven million which it was before. 

And now it's time to add our own classifier on top of this. So essentially we've got a pretty good Network right five by five by twelve 80s our last output. And what we want to do now is take that. And we want to use it to classify either cat or either dock. Right so what we're gonna do is add a global average layer which essentially is gonna take the entire average of every single sort of twelve hundred and eighty different layers that are five by five and put that into a 1d tensor which is kind of flattening that for us. So we do that global average pooling and then we're just gonna add the prediction layer which essentially is gonna just be one dense node. And since we're only classifying two different classes right dogs and cats. We only need one. Then we're gonna add all these models together. So the base model and is layers the global average layer that we define there and then the prediction layer to create our final model. So let's do this global average layer diction layer model give that a second to kind of run there now. When we look at the summary we can see we have mobile on that v2 which is actually a model. But that is our base layer. And that's fine is the output shape is that then global average pooling. 

Which again just takes this flattens it out does the average for us and then finally our dense layer which is going to simply have one neuron which is gonna be our output. Now notice that we have two point two five and nine million parameters in total and only 1281 of them are trainable that's because we have 1280 connections from this layer to this layer which means 1,280 weights and one bias. So that is what we're doing this is what we have created now this base the majority of the network has been done for us and we just add our own little classifier on top of this. And now we're gonna feed some training samples and data to this. Remember we're not training this base layer So the only thing that needs to be learned is the weights and biases on these two layers. Here once we have that we should have a decent model ready to go. So let's actually train this now. I'm gonna compile this. Here I'm picking a learning rate is very slow what essentially what the learning rate means is how much am I allowed to modify the weights and biases of this network. Which is what I've done just made that very low because we don't want to make any major changes if we don't have to because we're already using a base model that exists right. . 

So we'll set the learning rate. I'm not gonna talk about what this does. Specifically you can look that up if you'd like to. And then the loss function will use binary cross entropy just because we're using two classes. If you're using more than more than two classes you would just have cross entropy or some other type of cross entropy. And then what we're gonna do is actually evaluate the model right now before we even train it. So I've compiled it I've just set what we'll end up using but I want to evaluate the model currently without training it whatsoever on our validation data or validation batches and see what it actually looks like. 

What it actually you know what we're getting right now with the current base model being the way it is and not having changed the weights and biases they're completely random from the global average pooling in the dense layer. So let's evaluate let's see what we get as an accuracy. Okay so we can actually see that with the RAM weights and biases for those last layer that we added we're getting accuracy of 56 percent. Which pretty much means that it's guessing right. It's you know 50 percent is like two classes. So we got anything lower than 50 like 50 should have been our guest which is what we're getting so now what we're gonna do. And I actually I've trained this already. I think so I might not have to do it again is a train this model on all of our images - all of our images and cats and cats and dogs that we've loaded in before. 

Which will allow us now to modify these weights and biases of this layer. So hopefully it can determine what features need to be present for a dog to be a dog and for a cat to be a cat right. And then it can make a pretty good prediction. In fact I'm not gonna train this in front us right now because it actually takes close to an hour to train just because there is a lot of images that it needs to look at and a lot of calculations that need to happen. But when you do end up training this you end up getting an accuracy of a close to 92 or 93 percent which is pretty good. Considering the fact that all we did was use an original layer like base layer that classified up to a thousand different images so very general. 


And applied that just to cats and dogs by adding our dense layer classifier. On top so you can see this was kind of the accuracy. I had from training this previously I don't want to train again because it takes so long. But I did want to show that you can save a model and load a model by doing this syntax. So essentially on your model object you can call model dot save save it as whatever name you like. Dot h5 which is just a format for saving models and Chara's the specific to Kara's not tensorflow. And then you can load the model by doing this so. 

This is useful because after you train this for an hour obviously you don't want to retrain this if you don't have to do. To actually use it to make predictions so you can just load the model. Now I'm not gonna go into using them all specifically. You guys can look up the documentation to do that. We're at the point now where I've showed you so much syntax on predicting and how we actually use the models. But the basic idea would be to do model dot predict right and then you can see that it's even giving me the input here. 

So multi predict give it some X batch size or boast right because it will predict on multiple things. And that will spit back to you a class which then you can figure out. Ok this is a cat or this is a dog. You're gonna pass this obviously the same information we had before which is 160 by 160 by 3 and that will make the prediction for you. So that's kind of the thing there I was getting an OS error just because I hadn't saved this previously. But that's how you save and load models.

Which I think is important when you're doing the very large models. So when you fit this feel free to change the epochs to be something slower. If you'd like again right this takes a long time to actually end up running. But you can see that the accuracy increase is pretty black spandex 'pn entually from. When we didn't even have that classifier on. It now the last thing I want to talk about is object detection. I'm just gonna load up a page we're not gonna do any examples. I'm just gonna give you a brief introduction because we're kind of running out of time for this module. 

Because you can use tensorflow to do object detection and recognition which is kind of cool. So let's get into that now ok so right now I'm on a github page. That's built by tensorflow here. I'm gonna leave that link in the notebook. Where it said object detection. So you guys can look at that but essentially there is an API for tensorflow. That does object detection for you. And in fact it works very well and even gives you confidence scores. So you can see this is what you'll actually end up getting if you end up using this API. 

Now unfortunately we don't have time to go through this because this will take a good amount of time to talk. About the setup and how to actually use this project properly. But if you go through this documentation you should be able to figure it out And now you guys are familiar with tensorflow and you understand some of the concepts. Here this runs a very different model than what we've discussed before. Unfortunately again we don't time to get into it. But just something I wanted to make clear is that you can do something like this with tensorflow. And I will leave that resource. 

So that if you'd like to check this out you can use it. There's also a great module in Python called facial recognition it's not a part of tensorflow but it does use some kind of convolutional neural network to do facial detection and recognition which is pretty cool as well. so I'll put that link in here. But for that for now that's gonna be our. What is a convolutional neural network kind of module. So I hope that has cleared some things up on how deep vision works and how convolutional neural networks work. I know I haven't gone into crazy examples. 

What I've shown you some different techniques. That hopefully you'll go look up kind of on your own. And really dive into because now you have that base kind of domain knowledge. Where you're gonna be able to follow along with the tutorial and understand exactly what to do. And if you want to create your own model so long as you can get enough sufficient training you can load that training data into your computer. Put that in an umpire ray. Then what you can do is create a model like we've just done using even something like the mobile nets what does it V - that we talked to a previously. If I could even get up I need to close this output. Oh my gosh. 

This just this massive output here where is this they're gonna pre-trained model. Yeah mobile net B - you can use the base of that and then add your own classifier. On do a similar thing to what I've done with that dense neuron and that global average layer and hopefully you should get a decent result from that. So this is just showing you what you can do. Obviously you can pick a different base layer depending on what kind of problem you're trying to solve. Sometimes that has been convolutional neural networks. I hope you enjoyed that module. Now we're on to recurrent neural networks. Which is actually I mean pretty interesting. So I'll see you in that module. 



