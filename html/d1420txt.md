### TensorFlow - Natural Language Processing With RNNs: Create a Play Generator

So now we're on to our last and final example which is going to be creating a recurrent neural network play generator. Now this is gonna be the first kind of neural network we've done that's actually gonna be creating something for us. But essentially what we're gonna do is make a model that's capable of predicting the next character in a sequence. So we're gonna give it some sequence as an input and what it's gonna do is just simply predict the most likely next character. Now there's quite a bit that's gonna go into this but the way we're gonna use this to predict a play is we're going to train the model on a bunch of sequences of text from the play Romeo and Juliet. And then we're gonna have it so that we'll ask the model we'll give it some starting prompts some string to start with. And that'll be the first thing we pass to it it will predict to us with the most likely next character for that sequence is. 

And we'll take the output from the model and feed it as the input again to the model and keep predicting sequence of characters so keep predicting the next character from the previous output as many times as we want to generate an entire play. So we're gonna have this neural network that's capable of predicting one letter at a time actually end up generating an entire play for us by running it multiple times on the previous output from the last iteration. Now that's kind of the problem that's what we're trying to solve. So let's go ahead and get into it and talk about what's involved in doing this. So the first thing we're gonna do obviously is our imports so from cara's pre-processing import sequence import Chara's we need tensorflow numpy and OS. 

So we'll load that in and now what we're gonna do is download the file so the data set for Romeo and Juliet which we can get by using this line here. So Karras has this utils thing which will allow us to get a file save it as whatever we want in this case we're gonna save it as Shakespeare dot txt. And we're gonna get that from this link now I believe this is just some like shared drive that we have access to from cara's so we'll load that in here. And then this will simply give us the path on this machine because remember this is Google collaboratory to this txt file. Now if you want you can actually load in your own text data so we don't necessarily need to use this Shakespeare play. We could use anything we want in fact an example that I'll show later is using the beam a script. But the way you do that is run this block of code here. And you'll see that it pops up this thing for choose files just choose a file from your local computer and then what that will do is just save this on google collaboratory and then that will allow you to actually use that so make sure that's a text file that you're loading in there. But regardless that should work and then from there you'll be good to go so if you you know you don't need to do that you can just run this block of code here. 

If you want to load in the Shakespeare txt but otherwise you can load in your own file. Now after we do that what we want to do is actually open this file. So remember that was just saving the path to it so we'll open that file in our B mode which just read bytes mode I believe. And then we're gonna say dot read this we're gonna read that in as an entire string. We're gonna decode that into utf-8 format and then we're just printing the length of the text or the amount of characters in the text. 

So if we do that we can see we have the length of the text is 1.1 million characters approximately. And then we can have a look at the first 250 characters by doing this. So we can see that this is kind of what the plate looks like. We have whoever's speaking : then some line whoever is speaking cool in some line and there's all these brake lines so backslash ends. Which are telling us you know go to the next line. Right so it's gonna be important because we're gonna hope that our neural network will be able to predict things like brake lines and spaces. And even this kind of format as we teach it more. And get further in but now it's time to talk about encoding. So obviously all of this text is in text form it's not pre processed for us which means we need to pre-process it and encode it as integers before we can move forward. And fortunately for us this problem is actually little bit easier than the problem we discussed earlier with encoding words. 

Because what we're gonna do is simply encode each character in the text with an integer. Now you can imagine why this makes this easier because there really is a finite set of characters. Whereas there's kind of indefinite or you know I guess infinite amount of words that could be created. So we're not really gonna run into the problem we're you know two words are encoded with such differ or two characters are encoded with such different integers that it makes it difficult for the model understand. 

Because I mean we can look at what the value of vocab is here. We're only gonna have so many characters in the text and four characters it just doesn't matter as much. Because you know an R isn't like super meaningful compared to an A so we can kind of encode in a simple format. Which is what we're gonna do so essentially we need to figure out how many unique characters are in our vocabulary. So to do that we're gonna say vocab equals sorted set text this will sort all the unique characters in the text. And then what we're gonna do is create a mapping from unique characters to Indies indices so essentially we're gonna say you i4i you in a new enumerator vocabulary what this will do is give us essentially zero. 

Whatever the string is 1 whatever the string is to whatever the string is for every single letter or character in our vocabulary which will allow us to create this mapping. And then what we'll do is just turn this initial vocabulary into a list or into an array so we can just use the index at which a letter appears as the reverse mapping. So going from index to letter rather than lettered index which is what this one's doing here. Next I've just written a function that takes some text and converts that to an int or the interpretation for it just to make a little bit easier for us. As we get later on in the tutorial so we're just gonna say NP dot array of in this case. And we're just gonna convert every single character in our text into its integer representation by just referencing that character and putting that in a list here and then obviously converting that's a numpy array. 

So then if we wanted to have a look at how this works we can say text as int equals text to int text. So remember text is that entire loaded file that we had above here so we're just going to convert that to its integer representation entirely using this function. And now we can look at how this works down here so we can see that the text first citizen which is the first 13 letters is encoded by 1847 56 57 58 1. And obviously each character has its own encoding and you can go through and kind of figure out what they are based on the ones that are repeated. Right so that is how that works now I figured while we were at it we might as well write a function that goes the other way so into text reason. I'm try to convert this to an umpire a first is just because we're gonna be passing in different objects potentially in here. 

So if it's not already an umpire right it needs to be an umpire right. Which is kind of what this is doing otherwise we're just gonna pass on that we don't need to convert it to an umpire ray if it already is one we can just enjoy in all of the characters from this list into here. 

So that's essentially what this is doing for us just joining into text and then we can see if we go into text Texas int : 13 that translates that back to us for citizen I mean you can look more into this function. If you want but it's not that complicated okay so now that we have all this text encoded as integers what we need to do is create some training examples. 

It's not really feasible to just pass the entire you know 1.1 million characters to our --hour model at once for training. We need to split that up into something that's meaningful. So what we're actually gonna be doing is creating training examples where we have the first where the training input right so the input value is gonna be some sequence of some length. We'll pick the sequence length in this case we're actually gonna pick a hundred and then the output or the expected output so I guess like the label for that training example is gonna be the exact same sequence shifted right by one character. So essentially I put a good example here. Our input will be something like hell right and our output will be e llo so what its gonna do is predict this last character essentially. And these are what our training examples are going to look like so the entire beginning sequence and then the output sequence should be that beginning sequence - the first letter but tack on what the last letter should be. 

So that this way we can look at some input sequence and then predict that output sequence so you know plus a character. Right okay so that's how that works so now we're gonna do is define a sequence length of 100. We're gonna say the amount of examples per epoch is gonna be the length of the text divided by the sequence length plus 1 the reason. We're doing this is because for every training example we need to create a sequence input that's a hundred characters long and we need to create a sequence output that's a hundred characters long which means that we need to have a hundred and one characters that we used for every training example. Right hopefully that would make sense so what this next line here is gonna do is convert our entire string data set into characters. And it's actually going to allow us to have a stream of characters which means that it's gonna can essentially contain you know 1.1 million characters inside of this TF data set object from tensor slices. That's what that's doing next so let's run this and make sure this works all right. 

What we're gonna do is say sequences is equal to char data set batch sequence length is the length of each batch so in this case 101 and then drop remainder means let's say that we have you know a hundred and five characters in our text. Well since we need sequences of length 101 we'll just drop the last four characters of our text because we can't even put those into a batch so that's what this is doing for us is gonna take our entire character data set here that we've created and batch it into lengths of 101 and then just drop the remainder. 

So that's what we're gonna do here sequences does now split input target. What this is gonna do essentially is just create those training examples that we needed. So taking this these sequences of 101 lengths and converting them into the input and target text and I'll show you how they work in a second. We can do this convert the sequences to that by just mapping them to this function so that's what this function does so if we say sequences dot map and we put this function here. That means every single sequence will have this operation applied to it and that will be stored inside this data set object. 

Or I guess you could say object but we also just say that's it's gonna be you know the variable. Right so if we want to look at an example of how this works. We can kind of see. So I just as example the input will be first citizen before we proceed any further here me speak all speak speak first citizen U and the output notice the first character is gone starts at I and the last character is actually just a space here whereas here it didn't have a space. Where you can see there's no space here. There is a space that's kind of what I'm trying to highlight for you the next example we get our all resolved rather to die rather than fan whatever it goes to here right and then we can see here we omit that a and the next letter is actually a K right that's added in there. 

So that's how that works okay so next we need to make training batches so we're gonna say the batch size equals 64 the vocabulary size is the length of the vocabulary which if you remember all the way back up to the top of the code was the sets are the sorted set of the text which essentially told us how many unique characters are in there the embedding dimension is 256 the RNN units is a thousand 24 and the buffer size is 10,000 what we're gonna do. Now is create a data set that shuffled so we're gonna switch around all these sequences so they don't get shown in the proper order which we actually don't want. And we're gonna batch them by the batch size so if we haven't kind of gone over what batching and all this does before. I mean you can read these comments this is straight from the tensorflow documentation. 

What we want to do is feed our model 64 batches of data at a time so what we're gonna do is shuffle all the data batch it into that size and then again drop the remainder. 

If there's not enough batches which is what we'll do we're gonna define the embedding dimension which is essentially how big we want every single vector to represent our words are in the embedding layer. And then the RNN units I won't really discuss what that is right now. But that's essentially how many it's hard to really just I'm just gonna omit describing at for right now. Because I don't want to butcher an explanation it's not that important anyways okay. 





