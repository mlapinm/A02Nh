### Natural Language Processing With RNNs D - TensorFlow 2.0 Course

Okay so now we are on to our first example where we're gonna be performing sentiment analysis on movie reviews to determine whether they are positive reviews or negative reviews. 

Now we already know what sentiment means that's essentially what I just described so picking up you know whether a block of text is considered positive or negative. And for this example we're gonna be using the movie reviewed data sets. Now as per usual this is based off of this tensorflow tutorial slash guide. I found this one kind of confusing to follow in the tensorflow website. But obviously you can follow along with that if you don't prefer that version over mine. But anyways we're gonna be talking about the movie reviewed data set so this data set is straight from cara's and contains 25,000 reviews which are already pre processed and labeled. 

Now what that means for us is that every single word is actually already encoded by an integer and in fact they've done kind of a clever encoding system. Where what they've done is said if a character is encoded by say integer zero that represents how common that word is in the entire data set. So if an integer was encoded by or non integer a word was encoded by integer three that would mean that it is the third most common word in the data set. And in this specific data set we have a vocabulary size of eighty eight thousand five hundred eighty-four unique words. 

Which means that something that was classified as this so eighty eight thousand five hundred eighty four would be the least common word in the data set. So starting to keep in mind we're gonna load in the data set and do our imports just by hitting run here. 

And as I've mentioned previously you know I'm not gonna be typing this stuff out it's just it's kind of a waste of time. I don't have all the syntax memorized I would never expect you guys to memorize this either. But what I will do is obviously walk through the code step by step and make sure you understand why it is that we have what we have here. Okay so what we've done is to find the vocabulary size the max length of a review and the batch size. Now what we've done is just loaded in our data set by defining the vocabulary size so this is just the words that will include. 

So in this case all of them then we have train data train labels test data test labels and we can look at a review and see what it looks like by doing something like this. So this is an example of our first review. We can see kind of the different encodings for all of these words and this is what looks like they're already in integer form. Now just something to note here is that the length of our reviews are not unique. So if I do the Len of train data I guess I wouldn't say unique. But I mean they're just all different. So the line of train data 0 is different than the land I've trained out of one. Right so that's something to consider as we go through this. 

And something we're actually going to have to handle ok so more pre-processing. So this is what I was talking them if you have a look at our loaded interviews we'll notice they're of different lengths. This is an issue we cannot pass different length data into our neural network which is true therefore we must make each review the same length. Ok so what we're going to do for now is we're actually going to pad our sequences. Now what that means is we're gonna follow this kind of step. 

That I've talked about here so if the review is greater than 250 words we will trim off extra words if the review is less than 250 words we'll add the necessary amount of. This should actually be zeroes in here let's fix this zeroes to make it equal to 250. So what that means is we're essentially gonna add some kind of padding to our review. 

So in this case I believe we're actually gonna pad to the left side which means that say we have a review of length you know 200 we're gonna add 50 just kind of blank words which will represent with the index 0 to the left side of the review to make it the necessary length. So that's that's good we'll do that so if we look at train data and test out of what this does is we're just gonna use something from cara's which we've imported above. 

So we're saying from cara's the pre-processing import sequence again. We're treating our text data as a sequence. 

As we've talked about we're gonna say sequence top had sequences train data. And then we define the length that we want to pad it to. So that's what this will do it will perform these steps that we've already talked about. 

And again we're just going to assign test data and train data to you know whatever this does for us we can pass the entire thing it'll pad all of them for us at once. Ok so let's run that and then let's just have a look at say trained out of one. Now because remember this was like 189 right so if we look at train data so train underscored data. One like that we can see that as an array with a bunch of zeros before because that is the padding that we've employed to make it the correct length okay so that's padding that's something that we're probably gonna have to do most the time when we feed something to our neural networks. 

All right so the next step is actually to create the model. Now this model is pretty straightforward we have an embedding layer in LST M and a dense layer here. So the reason we've done dense with the activation function of sigmoid at the end is because we're trying to pretty much predict the sentiment of this right. Which means that if we have the sentiment between 0 & 1 then if a number is greater than 0.5 we could classify that as a positive review and if it's less than 0.5 or equal. You know whatever you want to set the bounds at then we could say that's a negative review so sigmoid as we probably might recall squishes our values between 0 & 1. So whatever the value is at the end of the network will be between 0 & 1. 

Which means that you know we can make the accurate prediction now here the reason we have the embedding layer you're like well we've already pre processed our review is even. Though we've pre-processed this with these integers and they are a bit more meaningful than just our random lookup table that we've talked to it before. We still want to pass that to an embedding layer which is going to find a way more meaningful representation for those numbers than just their integer values already. So it's gonna create those vectors for us and this 32 is denoting the fact that we're gonna make the output of every single one of our embeddings or vectors that are created 32 dimensions. Which means that when we pass them to the LST M layer we need to tell the LSD M layer it's gonna have 32 dimensions for every single word which is what we're doing. 

And this will implement that long short-term memory process we talked about before and output the final output to TF to Chara's that layers dense. Which will tell us you know that's what this is right. It'll make the prediction so that's what this model is we can see give us a second to run here the model summary which has already printed out. 

We can look at the fact that the embedding layer actually has the most amount of parameters because essentially it's trying to figure out you know all these different numbers. How can we convert that into a tensor of 32 dimensions which is not that easy to do. 

And this is gonna be the major aspect that's being trained. And then we have our lsdm layer we can see the parameters there and our final dense layer which is eight getting 33 parameters that's because the put from every single one of these dimensions 32 plus a bias node right that we need. So that's what we'll get there it seemed modeled on summary. Now we get the sequential model okay so training alright so now it's time to compile and train the model you can see. I've already trained mine what I'm gonna say here is if you want to speed up your training because this will actually take a second. And we'll talk about why we pick these things in a minute is go to run time change run time type. And add a hardware accelerator of GPU what this will allow you to do is utilize a GPU while you're training which should speed up your training by about 10 to 20 times. So I probably should have mentioned that beforehand but you can do that and please do for these examples. 

So model dot compiled. All right. So we're compiling our model we're picking the lost function as binary cross-entropy. The reason we're picking this is because this is going to essentially tell us how far away we are from the correct probability right because we have two different things we could be predicting so you know either 0 or 1. So positive or negative so this will give us a correct loss for that kind of problem that we've talked about before. The optimizer we're gonna use rmsprop again I'm not going to discuss all the different optimizers you can look them up if you care that much about what they do. And we're gonna use metrics as a cc. One thing I will say is the optimizer is not crazy important for this one. You can use Adam if you wanted to and it would still work fine. My usual go-to is just use the Adam optimizer unless you think there's a better one to use. 

But anyways that's something to mention ok so finally we will fit the model we've looked at this syntax a lot before so model up fit will give the training data the training labels the epochs. And we'll do a validation split of 20% so it's what 0.2 stands for which means that what we're gonna be doing is using 20% of the training data to actually evaluates and validate the model as we go through. And we can see that after training which I've already done and you guys are welcome to obviously do on your own computer. 

We kind of stall at an evaluation accuracy of about 88 percent whereas the model actually gets over fit to about 97 98 percent. So what this is telling us essentially is that we don't have enough training data and that after we've even done just one epoch we're pretty much stuck on the same validation accuracy. And that there's something that needs to to make it better but for now that's fine we'll leave it the way that it is okay. So now we can look at the results I've already did the results here just to again speed up some time. But we'll do the evaluation on our test data and test labels to get a more accurate kind of result here. 

And that tells us we have an accuracy of about eighty five point five percent. Which you know isn't great but it's decent considering. 

That we didn't really write that much code to get to the point that we're at right now.
