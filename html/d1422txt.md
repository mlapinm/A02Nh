### Natural Language Processing With RNNs: Training the Model

Okay so next we're going to compile the model which we'll do here. So we're gonna compile the model with the atom optimizer and the loss function. As loss which we defined here and now we're gonna set up some checkpoints. I'm not gonna talk about how these work. You can kind of just read through this if you want. And then we're going to train the model remember to start your GPU hardware accelerator under runtime change runtime type GPU because if you do not then this is gonna be very slow. But once you do that you can train the model i've already trained it. But if we go through this training we can see it's gonna say train for 172 steps it's gonna take about you know 30 seconds per epoch probably maybe a little bit less than that. 

And the more epochs you run this for the better it will get this is a different we're not likely gonna over fit here. So we can run this for like say 100 epochs if we wanted to for our case let's actually start by just training this on let's say 2 epochs just to see how it does and then we'll train it on like 10 20 40 50 and compare the results. But you'll notice the more epochs the better it's gonna get but just like for our case we'll start with 2 and then we'll work our way up. So while that trains will actually explain the next aspect of this without running the code. So essentially what we need to do after we've trained the model of initialize the weights and biases we need to rebuild it using a new batch size of 1 so remember the initial batch size was 64 which means that we'd have to pass it's 64 inputs or sequences for it to work properly but now what I've done is I'm gonna rebuild the model and change it to a batch size of 1. So that we can just pass it some sequence of whatever length we want and it will work so if we run this we've rebuilt the model with batch size 1. 

That's the only thing we've changed and now what I can do is load the weights by saying model dot load weights TF train the latest checkpoint checkpoint directory. And then build the model using the tensor shape one none. I know sounds strange this is how we do this rebuild the model. 1 nan is just saying expect the input 1 and then none means we don't know what the next dimension length will be. But here checkpoint directory is just we've defined where on our computer we're going to save these tensorflow checkpoints. 

This is just saying this is the was of the prefix we're gonna save the checkpoint with. So we're gonna do the checkpoint directory and then checkpoint epoch where epoch will stand for obviously whatever epoch we're on so we'll save checkpoint here we'll save a checkpoint at epoch one a checkpoint at epoch two to get the latest checkpoint. We do this and then if we wanted to load any intermediate checkpoint say like checkpoint ten which is what I've defined here we could use this block of code down here. And I've just hardwired the checkpoint that I'm loading by saying TF train load checkpoint whereas this one just gets the most reason. 

So we'll get the most recent which should be checkpoint two for me. And then what we're gonna do is generate the text so this function I'll dig into it in a second. But I just want to run and show you how this works. Cuz I feel like we've done a lot of work for not very many results. Right now and I'm just gonna type in the string Romeo and just show you that when I do this we give it a second and it will actually generate an output sequence like this. So we have Romeo oh give this is the beginning of our sequence that says Lady Capulet Foodmart own father gnomes come to those shalt right so it's like pseudo. English most of it are like kind of proper words but again this is because we train it on just two epochs. So I'll talk about how we build this in a second. But if you wanted a better output for this part then you would train this on more epochs. So now let's talk about how I actually generated that output. So we rebuilt the model to accept a batch size of one which means that I can pass it a sequence of any length and in fact what I start by doing is passing the sequence that I've typed in here which was Romeo. Then what that does is we run this function generate text. I just stole this from tensor flows website like I've steel in almost all of this code. And then we say the number of characters to generate is 800 the input evaluation which is now what we need to pre-process this text again so that this works properly we could use my little function or we can just write this line of code here which does with the function that I wrote does for us. So char to idea s4s and start string start string is what we typed in in that case Romeo. 

Then what we're gonna do is expand the dimensions so essentially turn just a list like this that has all these numbers no nine eight seven into a double List like this or just a nested list because that's what it's expecting as the input one batch one entry. Then what we do is we're gonna say the string that we want to store because we want to print this out at the end. Right we'll put in this text generated list temperature equals 1.0 what this will allow us to do is if we change this value to be higher. Well I mean you can read the comment here right low temperature results in more predictable text higher temperature results in more surprising text. 

So this is just a parameter to mess with if you want you don't necessarily need it and I would like I've just left mine at one. For now we're gonna start by resetting the states of the model this is because when we rebuild the model it's gonna have stored the last state that it remembered when it was training so we need to clear that before we pass the new input text to it. 

And we say for I in range number eight which means however many characters we want to generate which is 800 here. What we're gonna do is say predictions equals model input a vowel that's gonna start as the start string that's encoded. Right and then what we're gonna do is say predictions equals TF dots squeeze prediction 0. What this does is take our predictions which is going to be in a nested list and just removes that exterior dimension so we just have the predictions that we want. 

We don't have that extra dimension that we need to index again. And then we're gonna say using a categorical distribution to predict the character returned by the model. That's what it writes here we'll divide by the temperature if it's one that's not gonna do anything and we'll say predicted ID equals we'll sample whatever the output was from the model which is what this is doing. And then we're going to take that output so the predicted ID and we are going to add that to the input evaluation and then what we're gonna say is text generate dot append. And we're gonna convert the text that are integers now back into a string and return all of this now. 

I know this seems like a lot again this is just given to us by tensor float to you know create this aspect you can read through the comments yourself if you want to understand it more. But I think that was a decent explanation of what this is doing so yeah that is how we can generate you know sequences using recurrent neural network. Now what I'm gonna do is go to my other window here where I've actually typed all of the code just in full and do a quick summary of everything that we've done just cuz there was a lot that went on and then from there I'm actually gonna train this on a B movie script and show you kind of how that works in comparison to the Romeo and Juliet. 

Okay so what I'm in now is just the exact same notebook we have before but I've just pretty much copied all the text in here. Or it's the exact same code we had before. So we just don't have all that other text in between so I can kind of do a short summary of what we did as well as show you how this worked when I trained it on the B movie script. So I did mention and I was gonna show you that I'm not lying I will show you can see I've got B movie txt loaded in here. And in fact actually I'm gonna show you this script first to show you what it looks like. So this is what the B movie script looks like you can see it's just like a long you know script of text I just downloaded this for free off the internet. And it's actually not as long as the Romeo and Juliet play so we're not gonna get as good of results from our model but it should hopefully be okay so we just start and I'm just gonna do a brief summary and then I'll show you the results from the B movie script just so that people that are confused maybe you have something that wraps it up here. We're doing our imports I don't think I need to explain that this part up here is just loading in your file. 

Again I don't think I need to explain that then we're actually going to read the file so open it from our directory decode it into utf-8. We're going to create a vocabulary and encode all of the text that's inside of this file. Then what we're gonna do is turn all of that text yeah into you know the encoded version. We're writing a function here that goes the other way around. So from into text not from text to int. We're going to define the sequence length that we want to train with which will be sequence length of a hundred you can decrease this value if you want you go fifty go 20 it doesn't really matter. It's up to you it just that's gonna determine how many training examples you're gonna have right is the sequence length next. What we're gonna do is create a character data set from a tensor slices from text as int. Well this is going to do is just convert our entire text that's now an integer array into a bunch of Weiss's of characters. I'm so that's what this is doing here so are not slices what am i saying you're just gonna convert like split that entire array into just characters like that's pretty much what it's doing. 

And then we're gonna say sequences equals char data set batch which now is gonna take all those characters and batch them in lengths of 101 we're gonna do then is split all of that into the training examples so like this right hg l l and then ELO. We're gonna map this function to sequences which means we're gonna apply this to every single sequence and store that in data set. 

Then we're gonna define the parameters for our initial network and we're going to shuffle the data set and batch that into now 64 training examples then we're going to make the function that builds the model which i've already discussed. We're going to actually build the model starting with the batch size of 64 we're gonna create our loss function compile the model set our check points for saving and then train the model and make sure that we say check point callback as the check point callback for the model. Which means it's gonna save every epoch the weights that the model had computed at that epoch so after we do that then our models train so we've trained them all to you can see i train this on fifty epochs for the b movie script. 

And then we're gonna do is build the model now with the batch size of one so we can pass one example tune and get a prediction we're gonna load the most recent weights into our model from the checkpoint directory that we defined above. And then what we're gonna do is build the model and tell it to expect the shape one none as its initial input now none just means we don't know what that value is going to be but we know we're gonna have one entry. All right so now we have this generate text method or a function here which i've already kind of went through how that works and then we can see if i type an input string so we type you know input string let's say of hello and hit enter. We'll watch and we can see that the b movie you know trained model comes up with its output here now unfortunately the b movie script does not work as well as Romeo and Juliet that's just because Romeo and Juliet is a much longer piece of text. 

It's much better only its format get the idea here and it's kind of cool to see how this performs on different data so I would highly recommend that you guys find some training data that you could give this other.  Than just the Romeo and Juliet or maybe even try another play or something and see what you can get out of it also quick side note to make your model better increase the amount of epochs here ideally you want this loss to be as low as possible. You can see mine was still actually moving down at epoch fifty you will reach a point where the amount of epochs won't make a difference although with models like this the more epochs typically the better because it's difficult for it to kind of over fit. 

These all you want to do really is just kind of learn how the language works and then be able to replicate that to you almost right so that's a lot nicer and what more predictable um yeah you kind of kind of the idea here and with that being said I'm gonna say that this section is probably done. 

Now I know this was a long probably confusing section for a lot of you but this is you know what happens when you start getting into some more complex things in machine learning it's very difficult to kind of grasp and understand all these concepts in an hour. Of me just explaining them what I try to do in these videos is introduce you to the syntax show you how to get a working you know kind of prototype and hopefully give you enough knowledge to the fact where if you're confused by something that. I said you can go and you can look that up and you can figure out kind of the more important details for yourself because I really just I can't go into all you know the extremes in these videos so II always that has been this section I hope you guys enjoyed doing this. I thought this was pretty cool and in the next section we're going to be talking about reinforcement learning.


