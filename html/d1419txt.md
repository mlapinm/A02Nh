### Natural Language Processing With RNNs: Making Predictions

Okay so that's what we're getting the model's been trained again it's not too complicated. And now we're on to making predictions. So the idea is that now we've trained our model and we want to actually use it to make a prediction on some kind of movie review. So since our data was pre-processed when we gave it to the model. It means we actually need to process anything we want to make a prediction on in the exact same way we need to use the same lookup table we need to encode. It you know precisely the same otherwise when we give it to the model it's going to think that the words are different and it's not going to make an accurate prediction so what i've done here is i've made a function that will encode any text into um. What do you call the proper pre-processed kind of integers right just like our training data was pre-processed that's what this function is going to do for us is pre-process some line of text. 

So what i've done is actually got in the lookup table. So essentially the mappings from ibm ib imdb but could read that properly from that data set that we loaded earlier. So let me go see if i can find where i defined imdb you can see up here so keras.datasets import imdb just like we loaded it in we can also actually get all of the word indexes or that map we can actually print this out if we want to look at what it is after. But anyways we have that mapping which means that all we need to do is keras.preprocessing.txt.text to word sequence. 

What this means is give given some text convert all that text into what we call tokens which are just the individual words themself. And then what we're going to do is just use a kind of for loop inside of here that says word index at word if word in word index l0 for word. And tokens now what this means is essentially if the word that's in these tokens now is in our mapping. So in that vocabulary of 88 000 words then what we'll do is replace its location in the list with that specific word or with that specific integer that represents it. 

Otherwise we'll put zero just to stand for you know we don't know what this character is. And then what we'll do is return sequence.pad sequences and we'll pad this token sequence and just return actually the first index here. The reason we're doing that is because this pad sequences works on a list of sequences so multiple sequences. 

So we need to put this inside a list which means that this is going to return to us a list of lists  so we just obviously want the first entry because we only want you know that one sequence that we padded. So that's how this works. Sorry that's a bit of a mouthful to explain but you guys can run through and print this stuff out if you want to see how all of it works specifically um. But yeah so we can run this cell and have a look at what this actually does for us on some sample text. So that movie was just amazing so amazing. 

We can see we get the output that we were kind of expecting. So integer encoded words down here and then a bunch of zeros just for all the padding. Now while we were at it i decided why not why don't we make a decode function. So that if we have any movie review like this that's in the integer form we can decode that into the text value. So the way we're going to do that is start by reversing the word index that we just created. 

Now the reason for that is the word index we looked at. Which is this right goes from word to integer but we actually now want to go from integer to word so that we can actually translate a sentence. Right so what i've done is made this decode integers function. 

We've set the padding key as 0 which means if we see 0 that's really just means you know nothing's. There we're going to create a text string which we're going to add to. And i'm just going to say four num in integers integers is our input which will be a list that looks something like this or an array whatever you want to call it. We're going to say if number does not equal pad so essentially if the number is not zero right it's not padding. 

Then what we'll do is add the lookup of reverse word index num so whatever that number is into this new string plus a space. And then just return text colon negative one which means return everything except the last space that we would have added. 

