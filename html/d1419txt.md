### Natural Language Processing With RNNs: Making Predictions

Okay so that's what we're getting the model's been trained again it's not too complicated. And now we're on to making predictions. So the idea is that now we've trained our model and we want to actually use it to make a prediction on some kind of movie review. So since our data was pre-processed when we gave it to the model. It means we actually need to process anything we want to make a prediction on in the exact same way we need to use the same lookup table we need to encode. It you know precisely the same otherwise when we give it to the model it's going to think that the words are different and it's not going to make an accurate prediction so what i've done here is i've made a function that will encode any text into um. What do you call the proper pre-processed kind of integers right just like our training data was pre-processed that's what this function is going to do for us is pre-process some line of text. 

So what i've done is actually got in the lookup table. So essentially the mappings from ibm ib imdb but could read that properly from that data set that we loaded earlier. So let me go see if i can find where i defined imdb you can see up here so keras.datasets import imdb just like we loaded it in we can also actually get all of the word indexes or that map we can actually print this out if we want to look at what it is after. But anyways we have that mapping which means that all we need to do is keras.preprocessing.txt.text to word sequence. 

What this means is give given some text convert all that text into what we call tokens which are just the individual words themself. And then what we're going to do is just use a kind of for loop inside of here that says word index at word if word in word index l0 for word. And tokens now what this means is essentially if the word that's in these tokens now is in our mapping. So in that vocabulary of 88 000 words then what we'll do is replace its location in the list with that specific word or with that specific integer that represents it. 

Otherwise we'll put zero just to stand for you know we don't know what this character is. And then what we'll do is return sequence.pad sequences and we'll pad this token sequence and just return actually the first index here. The reason we're doing that is because this pad sequences works on a list of sequences so multiple sequences. 

So we need to put this inside a list which means that this is going to return to us a list of lists  so we just obviously want the first entry because we only want you know that one sequence that we padded. So that's how this works. Sorry that's a bit of a mouthful to explain but you guys can run through and print this stuff out if you want to see how all of it works specifically um. But yeah so we can run this cell and have a look at what this actually does for us on some sample text. So that movie was just amazing so amazing. 

We can see we get the output that we were kind of expecting. So integer encoded words down here and then a bunch of zeros just for all the padding. Now while we were at it i decided why not why don't we make a decode function. So that if we have any movie review like this that's in the integer form we can decode that into the text value. So the way we're going to do that is start by reversing the word index that we just created. 

Now the reason for that is the word index we looked at. Which is this right goes from word to integer but we actually now want to go from integer to word so that we can actually translate a sentence. Right so what i've done is made this decode integers function. 

We've set the padding key as 0 which means if we see 0 that's really just means you know nothing's. There we're going to create a text string which we're going to add to. And i'm just going to say four num in integers integers is our input which will be a list that looks something like this or an array whatever you want to call it. We're going to say if number does not equal pad so essentially if the number is not zero right it's not padding. 

Then what we'll do is add the lookup of reverse word index num so whatever that number is into this new string plus a space. And then just return text colon negative one which means return everything except the last space that we would have added. 

And then if i print the decode integers. We can see that this encoded um thing that we had before which looks like this gets encoded by the string. That movie was just amazing so may sorry not encoded decoded because this was the encoded form so that's how that works. Okay so now it's time to actually make a prediction so i've written a function here that will make a prediction on some piece of text as the movie review for us. And i'll just walk us through quickly how this works then i'll show us the actual output from our model uh you know making predictions like this so what we say is we'll take some parameter text which will be our movie review. 

And we're going to encode that text using the encode text function we've created above. So just this one right here that essentially takes our sequence of you know words we get the pre-processing so turn that into a sequence remove all the spaces what not you know get the words then we turn those into the integers we pad that we return That so here we have our proper pre-processed text then what we do is we create a blank numpy array that is just a bunch of zeros that's in the form 1 250. You're in that shape now the reason i'm putting that in that shape is because the shape that our model expects is something 250 which means some number of entries. And then 250 integers representing each word because that's the length of a movie review is what we've told the model is the length 250. 

So that's the length of the review. Then what we do is we put pred zero so that's what's up here equals the encoded text. So we just essentially insert our one entry into this uh this array we've created then what we do is say model.predict on that array and just return and print the result zero. Now that's pretty much all there is to it i mean that's how it works. The reason we're doing result zero is because again model is optimized to predict on multiple things. Which means like i would have to do you know list of encoded text which is kind of what i've done by just doing this prediction lines here. Which means it's going to return to me an array of arrays so if i want the first prediction i need to index 0. Because that will give me the prediction for our first and only entry. All right. 

So i hope that makes sense now we have a positive review i've written and a negative review. And we're just going to compare the analysis on both of them so that movie was so awesome i really loved it and would watch it again. Because it was amazingly great. And then that movie sucked. I hated it and wouldn't watch it again was one of the worst things i've ever watched. So let's look at this now and we can see the first one gets predicted at 72 percent positive whereas the other one is 23 percent positive. So essentially what that means is that you know if the lower the number the more negative we're predicting it is the higher the number the more positive we're predicting. 

It is if we wanted to not just print out this value and instead what we wanted to do was print out you know positive or negative. We could just make a little if statement that says if this number is greater than 0.5 say positive. Otherwise say not say negative. 

Right and i just want to show you that changing these reviews ever so slightly actually makes a big difference. So if i remove the word awesome so that movie was so. And then i run this. You can see that oh wow this actually increases and goes up to 84. Right so the presence of certain words in certain locations actually makes a big difference. And especially when we have a shorter length review right. If we have a longer length review it won't make that big of a difference. But even the removal of a few words here. And let's see so the removing the word awesome changed it by almost like 10 right now. 

















