So evaluating the model we want to evaluate the model. We can evaluate evaluate it now on the test images and test labels. We're obviously gonna get the same thing because the valuation is test images and test labels so we should get the same accuracy as 67 3/5 which we do right here. 


All right so there we go we get about 70%. Of you guys train this on 10 epochs you should get close to 70 I'm a little bit lower just because I didn't want to go that high and that is now the model. I mean we can use this if we want we can use predict. We could pass in some image and we could see the prediction for it. I'm not going to do that just because we've already talked about that enough.

And I want to get into some of the so evaluating the model. We want to evaluate the model. We can cooler stuff when we're working with smaller data sets. So the basic idea here is this is actually a pretty small data set. Right we use about 60,000 images and if you think about the amount of different patterns we need to pick up to classify. 

You know things like horses versus trucks that's a pretty difficult task to do which means that we need a lot of data. And in fact some of the best convolutional neural networks that are out there are trained on millions of pieces of you know sample information or data. So obviously we don't have that kind of data so how can we work with you know a few images maybe like a few thousand images and still get a decent model. Well the thing is you can't unless we use some of the techniques and other to show you so working with small data sets.

So just like I mentioned it's difficult to create a very good convolutional neural network from scratch if you're using a small amount of data. That is why we can actually emploice these techniques the first one data augmentation but also using pre trained models to kind of accomplish what we need to do in us. We're gonna be talking about now in this second part of the tutorial. 

We're gonna create another convolutional neural network so just to clarify this is created we've made the model up here already. This is all we need to do. To do it this is the architecture and this was just to get you familiar with the idea. So data augmentation so this is basically the idea. If you have one image we can turn that image into several different images and train and pass all those images to our our model. 

So essentially if we can rotate the image if we can flip it if we can stretch it compress it you know shift it sumit whatever it is and pass that to our model. 

It should be better at generalizing because we'll see the same image but modified and augmented multiple times which means that we can turn a data set they have 10,000 images into 40,000 images by doing four augmentations on every single image. 

Now obviously you still want a lot of unique images but this technique can help a lot and is used quite a bit because that allows our kind of model to be able to pick up images that maybe are orientated differently or zoomed in a bit or stretch something different right just better at generalizing. Which is the whole point so I'm not gonna go through this in to depth too much depth but this is essentially a script that does data augmentation for you.

We're gonna use this EMA image data generator from the Cara's pre-processing image module. We're gonna create a image data generator object now essentially what this allows us to do is specify some parameters on how we want to modify our image. 

In this case we have the rotation range some shifts sheer zoom horizontal flip and the mode now. I'm not going to go into how this works you can look at the documentation if you'd like. But essentially this will just allow us to augment our images. Now what I'm gonna do is pick one arbitrary image from the test image data set just our test image. 

I guess group of photos whatever you want to call it I'm gonna convert that to an image array which essentially takes it from the weird data set object that it kind of is and turns it into an umpire. Right then we're gonna reshape this so it's in the form 1 comma which essentially means one and then this will figure out what the rest of the shape should be. Oh sorry 1 and then plus the image shape which is whatever this shape is. 

So we'll reshape that and then what we're gonna do is we're gonna say for batch in dataflow Gendo. Talk about how that works in a second. Essentially this is just gonna augment the image for us and actually save it onto our drive. So in this instance what's gonna happen is this data Genda flow is going to take the image which we've created here right.

If we formatted it correctly by doing these two steps which you need to do beforehand it's gonna save this image as test JPEG and this will be the prefix which means it'll be some information. After and it will do this as many times until we break. So essentially given an image it will do test 1 test 2 test 3 test 4 test 5 with random augmentations using this until eventually we decide to break out of this. 

Now what I'm doing is just showing the image by doing this. And batch 0 is just showing us you know that first image in there. And that's kind of how this works. So you can mess with this script and figure out a way to use it. But I would recommend if you want to do data augmentation just look into image data generator. This is something that I just want to show you so you're aware of. 

And I'll just run it so you can see exactly how this works. so essentially given an image of a truck. What it will do is augment it in these different ways. You can see kind of the shifts the translations the rotations all of that. And we'll do actually a different image here to see what one looks like. It's just to image say 20 tip you get something different. So in this case I believe this is maybe like a deer rabbit or a dog or something.

I don't really know exactly what it is because it's so blurry. But you can see that's kind of the shifts. We're getting and it makes sense because you want to have images in different areas. So that we have a better generalization. All right so let's close that okay. So now we're gonna talk about using. Or sorry what is it pre trained models. Okay so we talked about data augmentation that's a great technique if you want to increase the size of your data set. But what if even after that we still don't have enough images in our dams. 

Well what we can do is use something called a pre trained model. Now companies like Google and you know tensorflow which is owned by Google make their own amazing convolutional neural networks. That are completely open-source that we can use so what we're gonna do is actually use part of a convolutional neural network. That they've trained already on I believe 1.4 million images. 


And we're just gonna use part of that model as kind of the base of our model. So that we have a really good starting point. And all we need to do is what's called fine-tune the last few layers of that network so that they work a little bit better for our purposes. So what we're gonna do is essentially say all right we have this model that Google's trained. 

They've traned on 1.4 million images. It's capable of classifying let's say a thousand different classes which is actually the example. We'll look at later. So obviously the beginning of that model is what's picking up on the smaller edges.

And you know kind of the very general things that appear in all of our images. So if we can use the base of that model so kind of the beginning of it. That does a really good job picking up on edges. And general things that will apply to any images. 

Then what we can do is just change the top layers of that model a tiny bit or add our own layers to it to classify for the problem that we want. And that should be a very effective way to use this pre-trained model. We're saying we're gonna use the beginning part. It's really good at kind of the generalization step.

Then we'll pass it into our own layers. That will do whatever we need to do specifically for our problem. That's what's like the fine-tuning step.

And then we should have a model that works pretty well in fact. That's what we're gonna do in this example. Now so that's kind of the point of what I'm talking about. Here is using part of a model that already exists. That's very good at generalizing and it's been trained on so many different images. And then we'll pass our own training data in.

We won't modify the beginning aspect of our neural network because it already works really well. We'll just modify the last few layers. That are really good at classifying for example just cats and dogs. Which is exactly the example we're actually gonna do here.

So I hope that makes sense. As we get through this should be cleared up a little bit. But using a pre trained model is now the section we're getting into. So this is based on this documentation. As always I'm referencing everything so you guys can go see that if you'd like do. 

Our imports like this. We're gonna load a data set. This actually takes a second to load the data set. I believe oh maybe not and essentially the problem we're doing is trying to classify dogs versus cats with a fair degree of accuracy. In fact we'd like to get above 90%.

So this is the data set we're loading in from tensorflow data sets as DFDS. This is kind of a weird way to load it in again. Stuff like this you just have to reference the documentation. I can explain it to you but it's not really gonna help.

When the next example is gonna be a different way of loading the data right. So long as you know how to get the data in the correct form. You can get it into some kind of numpy array. You can split it into training testing and validation data you should be okay. 


And if you're using a tensorflow data set it should tell you in the documentation how to load it in properly. So we'll load it in here. We're training 80% trained. I will go 10% for what is it raw validation and 10% for the testing data. So we've loaded that. And now what we're doing. Here is just real look at a few images.

So this actually creates a function. I know this is a weird thing. This is pretty unique to this example. It allows us to call this function with some integer essentially and get what the actual string representation of that is to the label for it. And what I'm doing here is just taking two images from our raw training data set and just splaying them. 

And you can see that's what we're getting here dog and dog. If I go ahead and take five we'll see these are what our images look like right. So here's an example of a dog we have a cats right and so on so forth you kind of get that you get the point there. Now notice though that these images are different dimensions.

In fact none of these images other than these two actually are the same dimension. At all actually I don't think these ones are either so obviously. There's a step that we need to do which is we need to scale all these images to be the same size so to do that what we're gonna do is write a little function like this. 

The essentially will return an image that is reshaped so I guess that is reshaped to the image size which I'm going to set at 160 by 160. Now we can make this bigger if we want. But the problem sometimes if you make an image that is bigger than like you want. To make your image bigger than most of your data set examples and that means you're gonna be really stretching a lot of the examples out and you're losing a lot of detail. 

So it's much better to make the image size smaller rather than bigger you might say well if you make it small. You're gonna lose detail too. But it's just it's better to compress it smaller than it is to go really big. Even just when it comes to the amount of training time and how complex the network's gonna be. 

So here's that something to consider. You can mess around with those when you're making your own networks. But again smaller is typically better in my opinion. You don't wanna go too small. But something that's like you know half the size. What an average image would be all right.


So we're gonna go format example. So we're gonna do is take an image and a label and what this will do is return to us just the reshaped image and label. So in this case we're going to cast which just means convert every single pixel in our image to be a float32 value. 

Because it could be integers we're then going to divide that by a hundred twenty seven point five which taken is exactly half of 255 and then subtract one. Then we're going to resize this image to beat the image size. So sorry the image will be resized to the image size of 160 by 160 and we'll return the new image and the label. So now we can apply this function all of our images using map.

If you don't want know what map is essentially it takes every single example in in this case gonna be raw train and applies the function to it which will mean that it will convert to raw train into images that are all resized to 160 by 160. And we'll do the same thing for validation and test. So run that know there. And now let's have a look at our images and see what we get. And there we are now. 

I've just messed up the color because I didn't add AC map thing which I think I needed where it was the C map anyways you know what that's fine for now. This is what our images look like this is the resize now we get all our images 160 by 160 and we are good to go alright. So now let's have a look at the shape of an original image versus.

Our new image so I mean this was just to prove that essentially our original shapes were like 260 to 409 by some random values. And they're all reshaping out of 160 160 by 3 3 obviously is the color channel of the image.
