### Natural Language Processing With RNNs: Part 2

Okay so I've drawn a pretty basic example right now that I'm gonna try to use to illustrate the idea of q-learning and talk about some problems with it. And how we can kind of combat those as we learn more about how cue learning works but the idea here is that we currently have three States and why what is happening why was that happening up at the top I don't know anyways. The idea is we have three states s1 s2 and s3 and at each state we have two possible actions that can be taken we can either stay in this state or we can move now what I've done is kind of just written some integers here that represent the reward. That we're going to get or that the agent is going to get such that it takes that action in a given state. 

So if we take the action here in s1 right of moving then we will receive a reward of 1 because that's what we've written here is the reward that we get from moving whereas if we stay we'll get a reward of 3. You know same concept here if we stay we get 2 if we move we get 1. And I think you understand the point so the goal of our agent to remember is to maximize its reward in the environment. And what we're gonna call the environment is this right here the environment is essentially defines the number of states the number of actions. And you know the way that the agent can interact with these states and these actions so in this case the agent can interact with the states by taking actions that change its state. 

Right so that's what we're getting at with this now what I want to do is show you how we use this cue table or learning this cute able to come up with kind of the almost. 

You know the model like the machine learning model that we're gonna use so essentially what we would want to have. Here is we want to have a kind of pattern in this table that allows our agent to receive the maximum door. So in this case we're gonna say that our agent will start at state s1 and obviously whenever we're doing this reinforcement learning we need to have some kind of start state that the agent will start in this could be a random state it could change but it doesn't just start in some state so in this case we're gonna say it starts at s1. Now when we're in s1 the agent has two things that it can do it can stay in the current state and receive a reward of 3 or can move and receive a reward of one right if we get to s2 in this state what can we do we can stay which means we receive a reward of two. 

Or we can move which means we get a reward of one and same thing for s3. We can stay we get a reward of four and we can move we get a reward of one. Now right now if we had just ran this one time and have the agents stay in each state like start in each unique state this is what the cue table we would get would look like because after looking at this just one time starting in each state. What the agent would be able to or I guess two times because it would have to try each action let's say we had the agents start in each state twice so it started in s1 twice it started s2 twice and it started an s3 twice and every time it started there it tried one of the different actions. So when it started in s1 it tried moving once and then it tried staying once. We would have a cue table that looks like this. 

Because what would happen is we would update values in our queue table to represent the reward we received when we took that action from that state. So we can see here that one we're in state s1 and we decide to stay what we did is we wrote a three inside of the state column because that is how much reward we received when we moved right same thing for state two when we moved for state two or have I guess sorry stayed when we stayed in state two we received a reward of two same thing for four. Now this is okay right this tells us kind of you know the optimal move to make in any state to receive the maximum reward but what if we introduced the idea that you know our agent we want it to receive the maximum total reward possible. 

Right so if it's in state one ideally we'd like it to move to state two and then move to states three and then just stay in state three because it will receive the most amount of reward well with the current table that we've developed. 

If we just follow this and we look at the table we say okay if we want to use this q-learning table. Now to you know move an agent around our level what we'll do is we'll say okay what state is it in if it's in state two we'll do stay because that's the highest reward that we have in this table. If that's the approach we used then we could see that if our you know agents start in state one or state to you would stay in what we call a local minima because it's not able to kind of realize from this state that it can move any further and receive a much greater reward. Right and that's kind of the concert we're going to talk about. 

As we implement and you know discuss further how cue learning works but hopefully this gives you a little bit of insight into what we do with this table. Essentially when we're updating these table values is when we're exploring this environment so when we explore this environment and we start in a state when we take an action to another state we observe the reward that we got from going there and we observe the state that we change to. Right so we observe the fact that in state 1 when we go to state 2 we receive the reward of 1 and what we do is we take that observation and we use it to update this cue table and the goal is at the end of all of these observations. And there could be millions of them that we have a cue table that tells us the optimal action to take in any single state so we're actually hard coding this kind of mapping that essentially just tells us given any state. 

actions that could be taken and just take the maximum action or the reward that's supposed to give I guess the action that's supposed to give the maximal reward and if we were to follow that on this we can see we get stuck in the local minima which is why we're gonna introduce a lot of other concepts so our reinforcement learning model and cue learning we have to implement the concept of being able to explore the environment not based on previous experiences right because if we just tell our model ok what we're gonna do is we're gonna start in all these different states we're gonna start in the start state and just start navigating around if we update our model immediately our update our cue table immediately and put this three here for stay we can almost guarantee that since this 3 is here when our model is 

Right because if we just tell our model ok what we're gonna do is. We're gonna start in all these different states we're gonna start in the start state and just start navigating around. If we update our model immediately our update our cue table immediately and put this three. 

Here for stay we can almost guarantee that since this 3 is here when our model is training. Right if it's using this Q table to determine what state to move to next when it's training and determining what to do it's just always gonna stay which means we'll never get a chance to even see what we could have gotten to at s3. So we need to kind of introduce some concept of taking random actions and being able to explore the environment more free. Before starting to look at these Q values and use that for the training so I'm actually gonna go back to my slides now to make sure I don't get lost cuz I think I was starting to ramble a little bit. There so we're gonna now talk about learning the cue table so essentially I showed you how we use that cue table which is given some state. 

We just look that state up in the cue table and then determine what the maximum reward we could get. By taking you know some action is and then take that action and that's how we would use the cue table later on when we're actually using the model. But when we're learning the cue table that's not necessarily what we want to do. We don't want to explore the environment by just taking the maximum reward we've seen so far and just always going that direction. We need to make sure that we're exploring in a different way. And learning the correct values for the cue table so essentially our agent learns by exploring the environment and observing the outcome slash reward from each action it takes in a given state which we've already said. But how does it know what action to take in each state when it's learning that's the question I need to answer for you now. 

Well there's two ways of doing this our agent can essentially you know use the current Q table to find the best action which is kind of what I just discussed so taking looking at the cue table looking up the state and just taking the highest reward or it can randomly pick a valid action. And our goal is gonna be when we create this Q learning algorithm to have a really great balance of these two where sometimes we use the Q table to define the best action and sometimes we take a random action. So that is one thing but now I'm just gonna talk about this formula for how we actually update Q values so obviously what's gonna end up happening in our Q learning is. 

We're gonna have an agent that's gonna be in the learning stage exploring the environment and having all these actions and all these rewards and all these observations happening. 

And it's gonna be moving around the environment by following one of these two kind of principles randomly picking a valid action or using the current Q table to find the best action. And when it gets into a neck new state and it you know moves from state to state it's gonna keep updating this Q table telling it you know this is what I've learned about the environment. I think this is a better move we're gonna update this value but how does it do that in a way that's gonna make sense because we can't just put you know the maximum value we got from moving otherwise we're gonna run into that issue which I just talked about where we get stuck in that local Maxima. Right I'm not sure if I called it minimum before but anyways it's local Maxima where we see this high reward. 

But that's preventing us if we keep taking that action from reaching a potentially higher reward in a different state. So the formula that we actually use to update the cue table is this so cue state action equals Q state action. And a state action is just referencing first the rows for the state and then the action as the column plus alpha times and then this is all in brackets right reward plus I believe this is gamma times max Q of new state minus Q state action. So what the heck does this mean what are these constants. What is all this we're gonna talk about the constants in a minute. But I want to yeah I want to explain this formula actually. So let's okay well I guess we'll go through the constants. 

It's hard to go through a complicated math formula so a stands for the learning rate and gamma stands for the discount factor. So alpha learning rate gamma discount factor now what is the learning rate well this is a little blurb on what this is but essentially the learning rate ensures that. We don't update our Q table too much on every observation so before right when I was showing you like this if we can go back to my Windows Inc why is this not working I guess I'm just not patient enough. Before when I was showing you all I did when I took an action was I looked at the reward that I got from taking that action and I just put that in my Q table. Right now obviously that is not an optimal approach to do this because that means that in the instance where we hit stay one well. I'm not gonna be able to get to this reward of four because I'm gonna throw that you know three in here. 

And I'm just gonna keep taking that action we need to. You know hopefully make this move action actually have a higher value than stay. So that next time we're in state one we consider the fact that we can move to state two and then move to state three to optimize a reward so how do we do that well the learning rate is one thing that helps us kind of accomplish. This behavior essentially what it is telling us and this is usually a decimal value right is how much we're allowed to update every single Q value by on every single action every single observation. 

So if we just use the approach before then we're only going to need to observe given the amount of states and the amount of actions.

And we'll be able to completely fill in the cue table so in our case if we had like three states and three actions we could you know nine iterations. We'd be able to fill the entire cue table. The learning rate means that it's gonna just update a little bit slower and essentially change the value in the cue table very slightly. So you can see that what we're doing is taking the current value of the cue table so whatever is already there and then what we're gonna do is add some value here and this value that we add is either gonna be positive or negative. Essentially telling us you know whether we should take this new action or whether we shouldn't take this new action now the way that this kind of value is calculated right is obviously our alpha is multiplied this by this. But we have the reward plus in this case gamma which is just gonna actually be the discount factor. 

And I'll talk about how that works in a second of the maximum of the new state we moved into now what this means is find the maximum reward that we could receive in the new state by taking any action and multiply that by what we call the discount factor. What this part of the formulas trying to do is exactly what I've kind of been talking about try to look forward and say okay so I know if I take this action in this state I receive this amount of reward. But I need to factor in the reward I could receive in the next state so that I can determine the best place to move to that's kind of what this Max and this gamma are trying to do for us. Or this discount factor whatever you want to call it it's trying to factor in a little bit about what we could get from the next state into this equation. 

So that hopefully our kind of agent can learn a little bit more about the transition states. So states that may be or actions that may be don't give us an immediate reward but lead to a larger reward in the future. That's what this Y and Max are trying to do then what we do is we subtract from this the state and action this is just to make sure that we're adding what the difference was in you know what we get from this versus. What the current value is and not like multiplying these values crazily I mean you can look into more of the math here and plug in like some values later and you'll see how this kind of works. But this is the basic format I feel like I explained that in depth enough. Okay so now that we've done that and we've updated this we've learned kind of how we update the cells and how this works. 

I could go back to the whiteboard and draw it out but I feel like that makes enough sense. We're gonna look at what the next state is we're gonna factor that into our calculation we have this learning rate which tells us essentially how much we can update each cell value by and we have this what do you call your discount factor which essentially tries to kind of define the balance between finding really good rewards in our current state and finding the rewards in the future state. So the higher this value is the more we're gonna look towards the future the lower it is the more we're gonna focus completely on our current reward right and obviously that makes sense. Because we're gonna add the maximum value and we're multiplying that by a lower number that means we're gonna consider that less than if that was greater awesome okay.


