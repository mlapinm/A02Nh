### TensorFlow - Natural Language Processing With RNNs: Building the Model

So now we're gonna go down to building the model. So we've kind of set these parameters up here. Remember what those are we've patched and we've shuffled the data set. And again that's how this works you can print it out if you want to see what a batch actually looks like but essentially it's just 64 entries of those sequences right so 64 different training examples is what a batch of. That is all right so now we go down here and we're gonna say build model we're actually making a function is going to return to us a built model. The reason for this is because right now we're gonna pass the model batches of size 64 for training right but what we're gonna do later is save this model. 

And then we're gonna patch pass it batches of one pieces of you know training whatever data so that it can actually make a prediction on just one piece of data. Cuz for right now what it's gonna do is takes a batch size of 64 it's gonna take 64 training examples and return to us 64 outputs. That's what this model is gonna be built to do the way. We build it now to start but later on we're gonna rebuild the model using the same parameters that we've saved and trained for the model but change it to just be a batch size of one so that that way we can get one prediction for one input sequence. Right so that's why I'm creating this build model function now in here. 

It's gonna have the vocabulary sizes first argument the embedding dimension which remember was 256 as a second argument. But also these are the parameters up here right. And then we're gonna define the batch size as you know batch size none. What this nun means is we don't know how long the sequences are gonna be in each batch. All we know is that we're gonna have 64 entries in each batch. And then of those 64 entries so the training examples right we don't know how long each one will be although in our case we're gonna use ones their length 100. But when we actually use the model to make predictions. We don't know how long the sequence is gonna be that we input so we leave this none. Next we'll make an LS TM layer which is long short term memory RNN units which is a thousand 24. 

Which again I don't really want to explain but you can look up if you want return sequences means return the intermediate stage at every step. The reason we're doing this is because we want to look at what the models seeing at the intermediate steps not just the final stage. So if you leave this as false and you don't set this to true what happens is this lsdm just returns one output that tells us what the model kind of found at the very last time step. 

But we actually want the output at every single time step for this specific model and that's why we're setting this true stateful. I'm not gonna talk about that one right now that's something you can look up if you want. And then recurrent initializer is just what these values are gonna start at in the lsdm. We're just picking this because this is what tensorflow is kind of set is a good default to pick. I won't go into more depth about that again things that you can look up more if you want. Finally we have a dense layer which is going to contain the amount of vocabulary size notes. The reason we're doing this is because we want the final layer to have the amount of nodes in it equal to the amount of characters in the vocabulary this way every single one of those nodes can represent a probability distribution that that character comes next.

So all of those nodes value some sum together should give us the value of one. And that's going to allow us to look at that last layer as a predictive layer where it's telling us the probability that these characters come. Next we've discussed how that's worked previously with other neural networks. So let's run this now named embedding dim is not defined. Which I mean believes I have not ran this yet so now we run that. And we should be good so if we look at the model summary we can see we have our initial embedding layer we have our LST M and we have our dense layer at the end. Now notice 64 is the batch size right that's the initial shape none is the length of the sequence which we don't know and then this is gonna be just the output dimension. I'm sorry this is the amount of values in the vector right so we're gonna start with 256 we'll just do a thousand 24 units in the LST m and then 65 stands for the amount of nodes cuz that is the length of the vocabulary. All right so combined that's how many trainable parameters we get. You can see each of them for each layer. 

And now it's time to move on to the next section. Okay so now we're moving on to the next step of the tutorial which is creating a loss function to compile our model with. Now I'll talk about why we need to do this in a second. But I first want to explore the output shape of our model. So remember the input to our model is something that is of length 64 because we're gonna have batches of 64 training examples. Right so every time we feed our model we're gonna give it 64 training examples. Now what those training examples are are sequences of length 100. 
So what I want to do now is actually look at the length of the example batch per day. And just print them out and look at what they actually are so example batch predictions is what happens. When I use my model on some random input example actually well the first one from my data set with when it's not trained. So I can actually use my model before it's trained with random weights and random biases and parameters by simply using model. And then I can put the little brackets like this and just pass in some example that. I want to get a prediction for so that's what I'm gonna do I'm gonna give it the first batch and you can even it shows me the shape of this batch 64/100. I'm gonna pass that to the model and it's gonna give us a prediction for that and in fact it's actually gonna give us a prediction for every single element in the batch right every single training example in the batch it's gonna give us a prediction for. So let's look at what those predictions are so this is what we get we get a length 64 tensor. Right and then inside of here we get a list inside of a list or an array inside of an array with all of these different predictions. 
That's what I want you to remember we're passing 64 entries that are all of length a hundred into the model as its training data. Right but some times and when we make predictions with the model later on we'll be passing it just one entry that is of some variable length right. And that's why we've created this build model function so we can build this model using the parameters that we've saved. Later on once we train the model and it can expect a different input shape right because when we're training it it's gonna be given a different shape than we're actually testing with it. Now what I want to do is explore the output of this model. Though at the current point in time so we've created a model that accepts a batch of 64 training examples that are length 100 so let's just look at what the output is from the final layer.

Give this a second to run we get 64 165 and that represents the batch size the sequence length and the vocabulary size. Now the reason for this is we have to remember when we create a dense layer as our last layer that has 65 nodes every prediction is going to contain 65 numbers. And that's gonna be the probability of every one of those characters occurring. Right that's what that does at the last one for us. So obviously our last dimension is gonna be 65 for the vocabulary size this is the sequence length. And that's imagine I just want to make sure this is really clear. Before we keep going otherwise it's gonna get very confusing very quickly. 

So we'll stop there for this like explaining this aspect here. But you can see we're getting 64 different predictions because there's 64 elements in the batch. Now let's look at one prediction so let's look at the very first prediction for say the first element in the batch. Right so let's do that here and we see now that we get a length a hundred tensor. And that this is what it looks like there's still another layer inside. And in fact we can see that there's another nested layer here at another nested array inside of this array. So the reason for this is because at every single time step which means the length of the sequence. Right because remember our recurrent neural network is gonna feed one at a time every word in the sequence. In this case our sequences are like the hundred at every time step. We're actually saving that output as a as a prediction right and we're passing that back so we can see that for one batch one training sorry not one batch one training example we had a hundred outputs. 

And these outputs are in some shape. We'll talk about what those are in a second. So that's something to remember that for every single training example we get whatever the length of that training example was out because that's the way that this model works. And then finally we look at the prediction at just the very first time step. So this is a hundred different time steps so let's look at the first time step and see what that prediction is. And we can see that now we get a tensor of length 65 and this is telling us the probability of every single character occurring next at the first time step. So that's what I want to walk through is showing you what's actually outputted from the model the current way that it works. And that's why we need to actually make our own loss function to be able to determine how you know good our models performing. When it outputs something ridiculous that looks like this because there is no just built-in loss function and tensorflow that can look at a three dimensional nested array of probabilities over. You know the vocabulary size and tell us how different the two things are so we need to make our own loss function. So if we want to determine the predicted character from this array so we'll go there now what we can do is get the categorical with this code. We can sample the categorical distribution and that will tell us the predicted character so what I mean is let's just look at this. And then we'll explain this so since our model works on random weights and biases. Right now we haven't trained it yet this is actually all of the predicted characters that it had so at every time step. At the first time step it predicted H then it predicted - than H then G then U and so on so forth you get the point. Right so what we're doing to get this value is we're gonna sample the prediction. 

So at this this is just the first time step actually more sample the prediction actually no sorry we're sampling every time step. My bad there we're gonna say sampled indices equals NP reshapes or just reshaping this just changing the shape of. It we're gonna say predicted characters equals int two texts sampled indices so it's I really it's hard to explain all this. Of you guys don't have those statistics kind of background a little bit to talk about. Why we're sampling and not just taking the argument max value of like this array. Because you would think that what we'll do is just take the one that has the highest probability out of here and now the index of the next predicted character. 

There's some issues with doing that for the loss function just because if we do that then what that means is we're gonna kind of get stuck in an infinite loop almost where we just keep accepting the biggest character. So what we'll do is pick a character based on this probability distribution kind of yeah again. It's hard it's called sampling the distribution you can look that up if you don't know what that means but sampling is just like trying to pick a character based on a probability distribution. It doesn't guarantee that the character with the highest probability is gonna be picked it just uses those probabilities to pick it. 

I hope that makes sense. I know that was like a really rambley definition but that's the best I can do. So here we reshape the array and convert all the integers to numbers to see the actual characters. So that's what these two lines are doing here and then I'm just showing the predicted characters by showing you this. And you know the character here is what was predicted at time step zero to be the next character. And so on okay so now we can create a loss function that actually handles this for us. So this is the loss function that we have Karis has like a built in one that we can utilize. Which is what we're doing but what this is gonna do is take all the labels and all of the probability distributions which is what this is logit. So i'm not going to talk about that really. And we'll compute a loss on those so how different or how similar those two things are remember the goal of our algorithm in the neural network is to reduce the loss. 


